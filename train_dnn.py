from dnn import DNN1, DNN2, DNN3, DNN4, DNN5

import torch
import argparse
import numpy as np
import pandas as pd
import torch.nn as nn
import torch.optim as optim
from prettytable import PrettyTable
from sklearn.preprocessing import Normalizer
from torch.utils.data import Dataset, DataLoader

np.random.seed(1337) # for reproducibility
torch.manual_seed(1337) # for reproducibility

class KDDDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels
        
    def __len__(self):
            return len(self.labels)

    def __getitem__(self, idx):
            label = self.labels[idx]
            data = self.data[idx]
            return data, label


def generate_train_test_dataset(trainfile, testfile):
    traindata = pd.read_csv(trainfile, header=None)
    testdata = pd.read_csv(testfile, header=None)

    X_train = np.array(traindata.iloc[:, 1:]).astype(float)
    y_train = np.array(traindata.iloc[:, 0]).astype(float).reshape(-1, 1)
    X_test = np.array(testdata.iloc[:, 1:]).astype(float)
    y_test = np.array(testdata.iloc[:, 0]).astype(float).reshape(-1, 1)

    X_train = Normalizer().fit(X_train).transform(X_train)
    X_test = Normalizer().fit(X_test).transform(X_test)
    
    X_train = torch.from_numpy(X_train)
    X_test = torch.from_numpy(X_test)
    y_train = torch.from_numpy(y_train)
    y_test = torch.from_numpy(y_test)

    traindataset = KDDDataset(X_train, y_train)
    testdataset = KDDDataset(X_test, y_test)

    return traindataset, testdataset


def train(model, device, trainloader, optimizer, criterion, batch_size, epochs, verbose):
    model.train()

    losses = []
    correct = 0
    for batch_id, batch_sample in enumerate(train_loader):
        data, target = batch_sample 
        data, target = data.float().to(device), target.float().to(device)
 
        optimizer.zero_grad()               # avoid gradient accumulation
        output = model(data)                # forward pass
        loss = criterion(output, target)    # compute loss
        loss.backward()                     # backward pass - compute gradients
        losses.append(loss.item())          # storing loss
        optimizer.step()                    # backward pass - update parameters

        threshold = torch.tensor([0.7])
        results = (output>threshold).float()*1
        correct += (results == target).sum().item()

    train_loss = float(np.mean(losses))
    train_acc = 100 * correct / len(train_loader.dataset)
    if verbose:
        print('\nTrain set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(
        train_loss, correct, len(train_loader.dataset), train_acc))

    return train_loss, train_acc


def test(model, device, testloader, optimizer, criterion, batch_size, epochs, verbose):
    model.eval()

    losses = []
    correct = 0
    with torch.no_grad():
        for batch_id, batch_sample in enumerate(test_loader):
            data, target = batch_sample 
            data, target = data.float().to(device), target.float().to(device)
    
            output = model(data)                # forward pass
            loss = criterion(output, target)    # compute loss
            losses.append(loss.item())          # storing loss
            
            threshold = torch.tensor([0.7])
            results = (output>threshold).float()*1
            correct += (results == target).sum().item()

    test_loss = float(np.mean(losses))
    test_acc = 100. * correct / len(test_loader.dataset)
    if verbose:
        print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset), test_acc))
    
    return test_loss, test_acc


def generate_parser():
    parser = argparse.ArgumentParser('DNN Exercise...')
    parser.add_argument('--model', type=int, default=1,
                        help='Select DNN model between 1-5.')
    parser.add_argument('--learning_rate', type=float, default=0.001,
                        help='Initial learning rate.')
    parser.add_argument('--num_epochs', type=int, default=10,
                        help='Number of epochs to run trainer.')
    parser.add_argument('--batch_size', type=int, default=64,
                        help='Batch size. Must divide evenly into the dataset sizes.')
    parser.add_argument('--verbose', type=bool, default=False,
                        help='Print each epoch values.')
    parser.add_argument('--trainfile', type=str, default='data/kddtrain.csv',
                        help='Train data file location.')
    parser.add_argument('--testfile', type=str, default='data/kddtest.csv',
                        help='Test data file location.')
    FLAGS = None
    FLAGS, unparsed = parser.parse_known_args()
    return FLAGS



def count_parameters(model):
    table = PrettyTable(["Modules", "Parameters"])
    hashmap = {}
    total_params = 0
    mp = {"model.0":"Dense-1", "model.3":"Dense-2", "model.6":"Dense-3", 
        "model.9":"Dense-4", "model.12":"Dense-5", "model.15":"Dense-6"}
    for name, parameter in model.named_parameters():
        if not parameter.requires_grad: continue
        params = parameter.numel()
        name = '.'.join(name.split('.')[:-1])
        if name not in hashmap:
            hashmap[name] = params
        else:
            table.add_row([mp[name], hashmap[name]+params])
        total_params+=params
    print(table)
    print(f"Total Trainable Params: {total_params}")
    return total_params
    

if __name__=="__main__":
    
    count_parameters(DNN5())

    FLAGS = generate_parser()
    use_cuda = torch.cuda.is_available()   
    device = torch.device("cuda" if use_cuda else "cpu") 
    traindataset, testdataset = generate_train_test_dataset(FLAGS.trainfile, FLAGS.testfile)
    
    train_loader = DataLoader(traindataset, batch_size = FLAGS.batch_size, 
                            shuffle=True, num_workers=4)
    test_loader = DataLoader(testdataset, batch_size = FLAGS.batch_size, 
                            shuffle=False, num_workers=4)
    
    models = ["", DNN1, DNN2, DNN3, DNN4, DNN5]
    model = models[FLAGS.model]().to(device)
    
    criterion = nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=FLAGS.learning_rate)

    best_accuracy = 0.0
    
    training_loss = []
    training_accuracy = []
    testing_loss = []
    testing_accuracy = []

    # Run training for n_epochs specified in config 
    for epoch in range(1, FLAGS.num_epochs + 1):
        train_loss, train_accuracy = train(model, device, train_loader, optimizer, 
                                    criterion, FLAGS.batch_size, epoch, verbose=FLAGS.verbose)
        test_loss, test_accuracy = test(model, device, test_loader, optimizer, 
                                    criterion, FLAGS.batch_size, epoch, verbose=FLAGS.verbose)

        training_loss.append(train_loss)
        training_accuracy.append(train_accuracy)
        testing_loss.append(test_loss)
        testing_accuracy.append(test_accuracy)
        
        if test_accuracy > best_accuracy:
            best_accuracy = test_accuracy
            path = "models/model_{}_epoch_{}".format(FLAGS.model, epoch)
            torch.save(model.state_dict(), path)    
        print("Model:", FLAGS.model, "Epoch: ", epoch)

    df = pd.DataFrame([training_loss, training_accuracy, testing_loss, testing_accuracy]).T
    df.columns = ["Train Loss", "Train Acc", "Test Loss", "Test Acc"]
    df.to_csv("DNN"+str(FLAGS.model)+"_"+str(FLAGS.num_epochs)+"_results.csv", index=False)
    print("Accuracy is {:2.2f}".format(best_accuracy))
    print("Training and evaluation finished")
    